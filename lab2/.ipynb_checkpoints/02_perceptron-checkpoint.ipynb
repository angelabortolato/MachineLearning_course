{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 2: Perceptrons\n",
    "\n",
    "Course 2024/25: *F. Chiariotti*\n",
    "\n",
    "The notebook contains a simple learning task over which we will implement **MODEL SELECTION AND VALIDATION**.\n",
    "\n",
    "Complete all the **required code sections** and **answer all the questions**.\n",
    "\n",
    "### IMPORTANT for the exam:\n",
    "\n",
    "The functions you might be required to implement in the exam will have the same signature and parameters as the ones in the labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Stayed/Churned Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Customer Churn table contains information on all 3,758 customers from a Telecommunications company in California in Q2 2022. Companies are naturally interested in churn, i.e., in which users are likely to switch to another company soon to get a better deal, and which are more loyal customers.\n",
    "\n",
    "The dataset contains three features:\n",
    "- **Tenure in Months**: Number of months the customer has stayed with the company\n",
    "- **Monthly Charge**: The amount charged to the customer monthly\n",
    "- **Age**: Customer's age\n",
    "\n",
    "The aim of the task is to predict if a customer will churn or not based on the three features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary Python libraries and load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "The dataset is a `.csv` file containing three input features and a label. Here is an example of the first 4 rows of the dataset: \n",
    "\n",
    "<center>\n",
    "\n",
    "Tenure in Months | Monthly Charge | Age | Customer Status |\n",
    "| -----------------| ---------------|-----|-----------------|\n",
    "| 9 | 65.6 | 37 | 0 |\n",
    "| 9 | -4.0 | 46 | 0 |\n",
    "| 4 | 73.9 | 50 | 1 |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "</center>\n",
    "\n",
    "Customer Status is 0 if the customer has stayed with the company and 1 if the customer has churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_dataset(filename):\n",
    "    data_train = pd.read_csv(filename)\n",
    "    #permute the data\n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True) # shuffle the data\n",
    "    X = data_train.iloc[:, 0:3].values # Get first two columns as the input\n",
    "    Y = data_train.iloc[:, 3].values # Get the third column as the label\n",
    "    Y = 2*Y-1 # Make sure labels are -1 or 1 (0 --> -1, 1 --> 1)\n",
    "    return X,Y\n",
    "\n",
    "# Load the dataset\n",
    "X, Y = load_dataset('data/telecom_customer_churn_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to differentiate (classify) between **class \"1\" (churned)** and **class \"-1\" (stayed)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the train set: 2817\n",
      "Number of samples in the test set: 940\n",
      "Number of churned users in test: 465\n",
      "Number of loyal users in test: 475\n",
      "Mean of the training input data: [ 0. -0. -0.]\n",
      "Std of the training input data: [1. 1. 1.]\n",
      "Mean of the test input data: [ 0.0134851   0.04850383 -0.0433016 ]\n",
      "Std of the test input data: [1.00014294 1.00683022 1.02078989]\n"
     ]
    }
   ],
   "source": [
    "# Compute the splits\n",
    "m_training = int(0.75*X.shape[0])\n",
    "\n",
    "# m_test is the number of samples in the test set (total-training)\n",
    "m_test =  X.shape[0] - m_training\n",
    "X_training =  X[:m_training]\n",
    "Y_training =  Y[:m_training]\n",
    "X_test =   X[m_training:]\n",
    "Y_test =  Y[m_training:]\n",
    "\n",
    "print(\"Number of samples in the train set:\", X_training.shape[0])\n",
    "print(\"Number of samples in the test set:\", X_test.shape[0])\n",
    "print(\"Number of churned users in test:\", np.sum(Y_test==-1))\n",
    "print(\"Number of loyal users in test:\", np.sum(Y_test==1))\n",
    "\n",
    "# Standardize the input matrix\n",
    "# The transformation is computed on training data and then used on all the 3 sets\n",
    "scaler = preprocessing.StandardScaler().fit(X_training) \n",
    "\n",
    "np.set_printoptions(suppress=True) # sets to zero floating point numbers < min_float_eps\n",
    "X_training =  scaler.transform(X_training)\n",
    "print (\"Mean of the training input data:\", X_training.mean(axis=0))\n",
    "print (\"Std of the training input data:\",X_training.std(axis=0))\n",
    "\n",
    "X_test =  scaler.transform(X_test)\n",
    "print (\"Mean of the test input data:\", X_test.mean(axis=0))\n",
    "print (\"Std of the test input data:\", X_test.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **homogeneous coordinates** to describe all the coefficients of the model.\n",
    "\n",
    "_Hint:_ The conversion can be performed with the function $hstack$ in $numpy$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_homogeneous(X_training, X_test):\n",
    "    #Transform the input into homogeneous coordinates\n",
    "    b_training=np.ones((X_training.shape[0],1))\n",
    "    b_test=np.ones((X_test.shape[0],1))\n",
    "    Xh_training = np.hstack((b_training,X_training))\n",
    "    Xh_test= np.hstack((b_test,X_test))\n",
    "    return Xh_training, Xh_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2817, 3) (940, 3)\n",
      "Training set in homogeneous coordinates:\n",
      "[[ 1.          1.2361321   0.87798477 -0.16001986]\n",
      " [ 1.          0.10884685  0.4417593   1.37363294]\n",
      " [ 1.          1.69539647 -1.57223186 -0.04204657]\n",
      " [ 1.          0.15059816 -0.93544295  0.84275312]\n",
      " [ 1.          0.56811122 -0.38890759 -0.57292638]\n",
      " [ 1.         -0.39216881 -1.41010975 -0.39596645]\n",
      " [ 1.         -1.0184384  -1.53880462 -1.04481955]\n",
      " [ 1.         -0.35041751 -0.71649454  0.72477983]\n",
      " [ 1.         -1.18544362 -1.45857925  0.4298466 ]\n",
      " [ 1.          1.44488863 -1.4385229  -0.16001986]]\n"
     ]
    }
   ],
   "source": [
    "# convert to homogeneous coordinates using the function above\n",
    "print(np.shape(X_training), np.shape(X_test))\n",
    "X_training, X_test = to_homogeneous(X_training, X_test)\n",
    "print(\"Training set in homogeneous coordinates:\")\n",
    "print(X_training[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic perceptron\n",
    "\n",
    "Now **complete** the function *perceptron*. <br>\n",
    "The **perceptron** algorithm **does not terminate** if the **data** is not **linearly separable**, therefore your implementation should **terminate** if it **reached the termination** condition seen in class **or** if a **maximum number of iterations** have already been run, where one **iteration** corresponds to **one update of the perceptron weights**. In case the **termination** is reached **because** the **maximum** number of **iterations** have been completed, the implementation should **return the best model** seen throughout.\n",
    "\n",
    "The current version of the perceptron is **deterministic**: we use a fixed rule to decide which sample should be considered (e.g., the one with the lowest index).\n",
    "\n",
    "The input parameters to pass are:\n",
    "- $X$: the matrix of input features, one row for each sample\n",
    "- $Y$: the vector of labels for the input features matrix X\n",
    "- $max\\_num\\_iterations$: the maximum number of iterations for running the perceptron\n",
    "\n",
    "The output values are:\n",
    "- $best\\_w$: the vector with the coefficients of the best model (or the latest, if the termination condition is reached)\n",
    "- $best\\_error$: the *fraction* of misclassified samples for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_errors(current_w, X, Y):\n",
    "    # This function:\n",
    "    # -computes the number of misclassified samples\n",
    "    n=0\n",
    "    # -returns the indexes of all misclassified samples\n",
    "    index=[]\n",
    "    # -if there are no misclassified samples, returns -1 as index\n",
    "    for idx in range(X.shape[0]):\n",
    "        if Y[idx]*(np.inner(current_w,X[idx, :]))<0 :\n",
    "            n+=1\n",
    "            index.append(idx)\n",
    "        if all(Y[idx]*(np.inner(current_w,X[idx, :]))>0):\n",
    "            index=-1\n",
    "    return n, index\n",
    "\n",
    "    \n",
    "def perceptron_fixed_update(current_w, X, Y):\n",
    "    # perceptron update function\n",
    "    (n, index)=count_errors(current_w, X, Y)\n",
    "    if index==-1 :\n",
    "        new_w=current_w\n",
    "    else :\n",
    "        for i in index:\n",
    "            new_w= current_w + Y[i]*X[i, :]\n",
    "    return new_w\n",
    "\n",
    "def perceptron_no_randomization(X, Y, max_num_iterations):\n",
    "    # write the perceptron main loop\n",
    "    current_w=0\n",
    "    best_w=0\n",
    "    best_error=2e9\n",
    "    for iters in range(max_num_iterations):\n",
    "        (n, index)=count_errors(current_w, X, Y)\n",
    "        if index==-1 :\n",
    "            break\n",
    "    \n",
    "    # The perceptron should run for up to max_num_iterations, or stop if it finds a solution with ERM=0\n",
    "    return best_w, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the implementation above of the perceptron to learn a model from the training data using 30 iterations and print the error of the best model we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_found, error = perceptron_no_randomization(X_training,Y_training, 30)\n",
    "print(\"Training Error of perceptron (30 iterations): \" + str(error))\n",
    "w_found2, error2 = perceptron_no_randomization(X_training,Y_training, 100)\n",
    "print(\"Training Error of perceptron (100 iterations): \" + str(error2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the best model $w\\_found$ to **predict the labels for the test dataset** and print the fraction of misclassified samples in the test set (the test error that is an estimate of the true loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_estimate(w,X,Y):\n",
    "    # Estimate the test loss\n",
    "    return t_loss_estimate\n",
    "\n",
    "\n",
    "true_loss_estimate =  loss_estimate(w_found, X_test, Y_test)       # Error rate on the test set\n",
    "true_loss_estimate2 =  loss_estimate(w_found2, X_test, Y_test) \n",
    "    \n",
    "print(\"Test Error of perceptron (30 iterations): \" + str(true_loss_estimate))\n",
    "print(\"Test Error of perceptron (100 iterations): \" + str(true_loss_estimate2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized perceptron\n",
    "\n",
    "Implement the correct randomized version of the perceptron such that at each iteration the algorithm picks a random misclassified sample and updates the weights using that sample. The functions will be very similar, except for some minor details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_randomized_update(current_w, X, Y):\n",
    "    # TODO: write the perceptron update function\n",
    "    return new_w\n",
    "\n",
    "def perceptron_with_randomization(X, Y, max_num_iterations):\n",
    "    # TODO: write the perceptron main loop\n",
    "    # The perceptron should run for up to max_num_iterations, or stop if it finds a solution with ERM=0\n",
    "    return best_w, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the correct version of the perceptron using 30 iterations and print the error of the best model we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the perceptron for 30 iterations\n",
    "w_found, error = perceptron_with_randomization(X_training, Y_training, 30)\n",
    "w_found2, error2 = perceptron_with_randomization(X_training, Y_training, 100)\n",
    "print(\"Training Error of perceptron (30 iterations): \" + str(error))\n",
    "print(\"Training Error of perceptron (100 iterations): \" + str(error2))\n",
    "\n",
    "true_loss_estimate =  loss_estimate(w_found, X_test, Y_test)       # Error rate on the test set\n",
    "true_loss_estimate2 =  loss_estimate(w_found2, X_test, Y_test) \n",
    "\n",
    "print(\"Test Error of perceptron (30 iterations): \" + str(true_loss_estimate))\n",
    "print(\"Test Error of perceptron (100 iterations): \" + str(true_loss_estimate2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Plot the loss with respect to the number of iterations (up to 1000)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
